TODOs

change tpdd_read() and file_to_shex() to read in chunks by delimiter (null)
instead of one byte at a time.

lcmd_llm() shows an example using mapfile() to get the entire file into ram
in one shot. That creates an array where each element has a random sized chunk
of data.

lcmd_lls() shows an example using read() to work on only one chunk at a time.

In either case, to reconstruct the file, concat the chunks plus one 0x00 between each.

start of file and end of file will need special handling
to detect if the first or last byte happened to be a 0x00

----

Replace TPDD1 "mv" with one that edits the FCB instead of load>rm>save.

It might not be much faster, because it would still be doing at least one
read-logical & write-logical, maybe a whole read-physical & write-physical.

Another point against is it risks all files since you have to overwrite
the entire FCB table, while deleting and saving a file only risks that file.

But the option should still exist because why not?.
We have the read_fcb() routine now to start from.

It would definitely be significantly faster for large files. Even merely
deleting a large file, without the load or save, takes a long time because
it causes the drive to have to edit a lot of sector IDs and FCBs. Plus
of course load and save each take a long time for a large file.

----

load/save/rr/rd in chunks instead of entire files / entire disks to ram

open a local file for reading or writing with a file descriptor <> redirect
and read/write the individual 128-byte chunks as they come.

point would be to avoid needing 600Kbyte variables (200k for an entire
tpdd2 disk, X2 from the hex encoding, plus probably at least one more byte
per array element for a null terminator internal to bash.)

Some routines already do this. lcmd_load() already does this.

Or conversely maybe we need to do everything from ram as much as possible
for timing reasons on different machines / os's.

----

auto baud detect?

----

TS-DOS / DeskLink directories

----

disk tester

stress-test disk to identify old media that's bad or too marginal
use sector acces to fill every byte with random binary, read back & compare, repeat.

----

Figure out a way to work on windows

possible workaround:
cmd /c mode COM6 baud=19200

https://stackoverflow.com/questions/42260492/cygwin-serial-port-listed-under-dev-but-stty-reports-invalid-argument

Or, since dlplus works, maybe some other app that's already available in cygwin like minicom or even uucp?
If all else fails, maybe we can include a tiny c program to build and run on the fly? Or even an encoded binary?
Another similar question the user claimed that python worked fine, just bash/stty failed, so maybe a python one-liner with no file just a commandline?

----

undelete()

When the drive deletes a file, it only edits the FCB and SMT. It leaves behind all the data and the sector chain pointers in the ID sections.

* Scan the FCB to get a list of all used head & tail sectors.
* Scan all ID's
* Walk all occupied sector chains from heads to tails to block out all occupied sectors that are actually mentioned in the FCB
* Out of the remaining unaccounted sectors, reconstruct any chains.

For each reconstructed chain:
* Note the head and tail sector
* Fabricate or ask a filename and attr. Assume a name with a filename pattern and attr according to the current COMPAT mode.
* For the total file size, a few possible answers:
  - if compat raw, ask user: end at end of tail sector, or last non-null byte, or user-supplied total file size.
  - if compat wp2, investigate wp-2 file formats
  - if compat floppy, .BA and .DO end at Ctrl-Z, .CO 6-byte header has length
* Write FCB and update SMT

----

FCB editor

----

SMT editor

----

ID editor

----

sector editor
