TODOs

change tpdd_read() and file_to_shex() to read in chunks by delimiter (null)
instead of one byte at a time.

lcmd_llm() shows an example using mapfile() to get the entire file into ram
in one shot. That creates an array where each element has a random sized chunk
of data.

lcmd_lls() shows an example using read() to work on only one chunk at a time.

In either case, to reconstruct the file, concat the chunks plus one 0x00 between each.

----

Replace TPDD1 "mv" with one that edits the FCB instead of load>rm>save.

It might not be much faster, because it would still be doing at least one
read-logical & write-logical, maybe a whole read-physical & write-physical.

Another point against is it risks all files since you have to overwrite
the entire FCB table, while deleting and saving a file only risks that file.

But the option should still exist because why not?.
We have the read_fcb routine now to start from.

It would definitely be significantly faster for large files. Even merely
deleting a large file without the load or save takes a long time because
it causes the drive to have to edit a lot of sector IDs and FCBs. Plus
of course load and save each take a long time for a large file.

----

load/save/rr/rd in chunks instead of entire files / entire disks to ram

open a local file for reading or writing with a file descriptor <> redirect
and read/write the individual 128-byte chunks as they come.

point would be to avoid needing 600Kbyte variables (200k for an entire
tpdd2 disk, X2 from the hex encoding, plus probably at least one more byte
per array element for a null terminator internal to bash.)

Some routines already do this. lcmd_load() already does this.

----

auto baud detect?

----

TS-DOS / DeskLink directories

----

disk tester

stress-test disk to identify old media that's bad or too marginal
use sector acces to fill every byte with random binary, read back & compare, repeat.

----

disk cleaner

just trigger any ordinary command that spins the disk
but ignore the error from no disk
